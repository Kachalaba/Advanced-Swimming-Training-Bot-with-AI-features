"""Detect swimmer and lane in video frames using YOLO."""

from __future__ import annotations

import json
import logging
from pathlib import Path
from typing import Dict, List

import cv2
import numpy as np
from ultralytics import YOLO

logger = logging.getLogger(__name__)


class SwimmerDetector:
    """Detect swimmer position in swimming pool frames."""

    def __init__(self, model_name: str = "yolov8n.pt"):
        """Initialize swimmer detector.

        Args:
            model_name: YOLO model name (n/s/m/l/x)
        """
        self.model = YOLO(model_name)
        # Person class ID in COCO dataset
        self.person_class_id = 0

    def detect_swimmer(
        self,
        frame_path: str,
        confidence_threshold: float = 0.3,
    ) -> Dict:
        """Detect swimmer in a single frame.

        Args:
            frame_path: Path to frame image
            confidence_threshold: Minimum confidence for detection

        Returns:
            Dict with:
                - bbox: [x1, y1, x2, y2] or None
                - confidence: float or None
                - center: [cx, cy] or None
                - lane: estimated lane number (1-8) or None
        """
        # Load frame
        frame = cv2.imread(frame_path)
        if frame is None:
            logger.error(f"Cannot read frame: {frame_path}")
            return {"bbox": None, "confidence": None, "center": None, "lane": None}

        height, width = frame.shape[:2]

        # Run detection
        results = self.model(frame, verbose=False)

        # Find person with highest confidence
        best_detection = None
        best_conf = 0

        for result in results:
            boxes = result.boxes
            for box in boxes:
                cls = int(box.cls[0])
                conf = float(box.conf[0])

                if cls == self.person_class_id and conf >= confidence_threshold:
                    if conf > best_conf:
                        best_conf = conf
                        x1, y1, x2, y2 = map(int, box.xyxy[0])
                        best_detection = {
                            "bbox": [x1, y1, x2, y2],
                            "confidence": conf,
                        }

        if best_detection is None:
            return {"bbox": None, "confidence": None, "center": None, "lane": None}

        # Calculate center
        bbox = best_detection["bbox"]
        cx = (bbox[0] + bbox[2]) // 2
        cy = (bbox[1] + bbox[3]) // 2
        best_detection["center"] = [cx, cy]

        # Estimate lane (divide width into 8 lanes)
        lane = min(8, max(1, int((cx / width) * 8) + 1))
        best_detection["lane"] = lane

        return best_detection

    def detect_batch(
        self,
        frame_paths: List[str],
        confidence_threshold: float = 0.3,
        enable_tracking: bool = True,
    ) -> List[Dict]:
        """Detect swimmer in multiple frames with optional tracking.

        Args:
            frame_paths: List of frame paths
            confidence_threshold: Minimum confidence
            enable_tracking: If True, track same swimmer across frames

        Returns:
            List of detection results for each frame
        """
        if not enable_tracking:
            # Simple per-frame detection
            results = []
            for i, frame_path in enumerate(frame_paths):
                detection = self.detect_swimmer(frame_path, confidence_threshold)
                detection["frame_index"] = i
                detection["frame_path"] = frame_path
                results.append(detection)
            return results

        # Tracking mode: maintain swimmer identity across frames
        return self._detect_with_tracking(frame_paths, confidence_threshold)

    def _detect_with_tracking(
        self,
        frame_paths: List,
        confidence_threshold: float = 0.5,
    ) -> List[Dict]:
        """Detect and track the same swimmer across frames.
        
        Uses IoU and distance to maintain swimmer identity when multiple people present.
        
        Args:
            frame_paths: List of frame paths (str) or frame dicts with 'path' and 'timestamp'
            confidence_threshold: Minimum confidence
            
        Returns:
            List of detection results tracking the same swimmer
        """
        results = []
        prev_bbox = None
        prev_center = None
        prev_prev_center = None  # –î–ª—è —Ä–∞—Å—á—ë—Ç–∞ velocity
        target_lane = None
        velocity = [0, 0]  # –°–∫–æ—Ä–æ—Å—Ç—å –¥–≤–∏–∂–µ–Ω–∏—è [vx, vy] –≤ –ø–∏–∫—Å–µ–ª—è—Ö/–∫–∞–¥—Ä
        frame_width = None  # –®–∏—Ä–∏–Ω–∞ –∫–∞–¥—Ä–∞ (–ø–æ–ª—É—á–∏–º –∏–∑ –ø–µ—Ä–≤–æ–≥–æ –∫–∞–¥—Ä–∞)
        
        for i, frame_info in enumerate(frame_paths):
            # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Å—Ç–∞—Ä–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ (str) –∏ –Ω–æ–≤–æ–≥–æ (dict)
            if isinstance(frame_info, dict):
                frame_path = frame_info["path"]
                timestamp = frame_info.get("timestamp")
                video_frame = frame_info.get("video_frame")
            else:
                frame_path = frame_info
                timestamp = None
                video_frame = None
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä –∫–∞–¥—Ä–∞ –µ—Å–ª–∏ –µ—â—ë –Ω–µ –ø–æ–ª—É—á–∏–ª–∏
            if frame_width is None:
                temp_frame = cv2.imread(frame_path)
                if temp_frame is not None:
                    frame_width = temp_frame.shape[1]
                else:
                    frame_width = 1920  # Fallback
            
            # Get all person detections in frame
            all_detections = self._detect_all_persons(frame_path, confidence_threshold)
            
            if not all_detections:
                # YOLO –Ω–µ –Ω–∞—à—ë–ª –ø–ª–æ–≤—Ü–∞ - –ø—ã—Ç–∞–µ–º—Å—è underwater detection
                underwater_det = None
                
                # –ü–æ–ø—ã—Ç–∫–∞ 1: Motion-based detection (–µ—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–µ–¥—ã–¥—É—â–∏–π –∫–∞–¥—Ä)
                if prev_bbox and i > 0:
                    # –†–∞—Å—à–∏—Ä—è–µ–º search bbox –Ω–∞ –æ—Å–Ω–æ–≤–µ velocity
                    search_margin = 100
                    if velocity and (velocity[0] != 0 or velocity[1] != 0):
                        # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –≥–¥–µ –∏—Å–∫–∞—Ç—å
                        predicted_center = [
                            prev_center[0] + velocity[0],
                            prev_center[1] + velocity[1]
                        ]
                        search_bbox = [
                            predicted_center[0] - (prev_bbox[2] - prev_bbox[0]) // 2 - search_margin,
                            predicted_center[1] - (prev_bbox[3] - prev_bbox[1]) // 2 - search_margin,
                            predicted_center[0] + (prev_bbox[2] - prev_bbox[0]) // 2 + search_margin,
                            predicted_center[1] + (prev_bbox[3] - prev_bbox[1]) // 2 + search_margin,
                        ]
                    else:
                        # –ë–µ–∑ velocity - –∏—â–µ–º –≤–æ–∫—Ä—É–≥ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –ø–æ–∑–∏—Ü–∏–∏
                        search_bbox = [
                            prev_bbox[0] - search_margin,
                            prev_bbox[1] - search_margin,
                            prev_bbox[2] + search_margin,
                            prev_bbox[3] + search_margin,
                        ]
                    
                    prev_frame_info = frame_paths[i - 1]
                    prev_frame_path_str = prev_frame_info["path"] if isinstance(prev_frame_info, dict) else prev_frame_info
                    
                    underwater_det = self._detect_underwater_with_motion(
                        frame_path,
                        prev_frame_path_str,
                        search_bbox
                    )
                    
                    if underwater_det:
                        logger.info(f"Frame {i}: üåä Underwater motion detection SUCCESS")
                
                # –ü–æ–ø—ã—Ç–∫–∞ 2: YOLO —Å –ø–æ–Ω–∏–∂–µ–Ω–Ω—ã–º confidence (0.15 –≤–º–µ—Å—Ç–æ 0.3)
                if not underwater_det:
                    ultra_low_detections = self._detect_all_persons(frame_path, confidence_threshold=0.15)
                    if ultra_low_detections:
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–ª–∏–∂–∞–π—à—É—é –∫ predicted position
                        if prev_center and velocity:
                            predicted_center = [
                                prev_center[0] + velocity[0],
                                prev_center[1] + velocity[1]
                            ]
                            best_det = min(ultra_low_detections, key=lambda d: 
                                np.sqrt((d["center"][0] - predicted_center[0])**2 + 
                                       (d["center"][1] - predicted_center[1])**2))
                            underwater_det = best_det
                            underwater_det["ultra_low_confidence"] = True
                            logger.info(f"Frame {i}: üåä Ultra-low confidence YOLO SUCCESS (conf={best_det['confidence']:.2f})")
                
                # –ü–æ–ø—ã—Ç–∫–∞ 3: Aggressive extrapolation –ø–æ velocity
                if not underwater_det and prev_bbox and prev_center and velocity:
                    if velocity[0] != 0 or velocity[1] != 0:
                        # –°—á–∏—Ç–∞–µ–º —Å–∫–æ–ª—å–∫–æ –∫–∞–¥—Ä–æ–≤ –ø–æ–¥—Ä—è–¥ –ø—Ä–æ–ø—É—â–µ–Ω–æ
                        frames_missing = 1  # –ü–æ–∫–∞ –ø—Ä–æ—Å—Ç–æ 1
                        underwater_det = self._aggressive_extrapolate(
                            prev_bbox, prev_center, velocity, frames_missing,
                            target_lane=target_lane,  # –ü–µ—Ä–µ–¥–∞—ë–º –∏–∑–≤–µ—Å—Ç–Ω—É—é –¥–æ—Ä–æ–∂–∫—É
                            frame_width=frame_width   # –ü–µ—Ä–µ–¥–∞—ë–º —à–∏—Ä–∏–Ω—É –∫–∞–¥—Ä–∞
                        )
                        logger.info(f"Frame {i}: ‚ö° Aggressive extrapolation USED")
                
                # –ï—Å–ª–∏ –≤—Å—ë —Ä–∞–≤–Ω–æ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏ - –ø—É—Å—Ç–∞—è –¥–µ—Ç–µ–∫—Ü–∏—è
                if not underwater_det:
                    logger.warning(f"Frame {i}: ‚ùå No detection (YOLO, motion, extrapolation all failed)")
                    results.append({
                        "frame_index": i,
                        "frame_path": frame_path,
                        "timestamp": timestamp,
                        "video_frame": video_frame,
                        "bbox": None,
                        "confidence": None,
                        "center": None,
                        "lane": None,
                    })
                    continue
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º underwater detection
                best_det = underwater_det
            else:
                # YOLO –Ω–∞—à—ë–ª –¥–µ—Ç–µ–∫—Ü–∏–∏ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–π —Ç—Ä–µ–∫–∏–Ω–≥
                # First frame: pick largest/most centered detection
                if prev_bbox is None:
                    best_det = self._pick_initial_swimmer(all_detections)
                    target_lane = best_det.get("lane")
                else:
                    # –í—ã—á–∏—Å–ª—è–µ–º velocity –µ—Å–ª–∏ –µ—Å—Ç—å –∏—Å—Ç–æ—Ä–∏—è –¥–≤–∏–∂–µ–Ω–∏—è
                    if prev_center and prev_prev_center:
                        velocity = [
                            prev_center[0] - prev_prev_center[0],  # vx
                            prev_center[1] - prev_prev_center[1],  # vy
                        ]
                    
                    # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Å–ª–µ–¥—É—é—â—É—é –ø–æ–∑–∏—Ü–∏—é –∏—Å–ø–æ–ª—å–∑—É—è velocity
                    predicted_center = None
                    if prev_center and (velocity[0] != 0 or velocity[1] != 0):
                        predicted_center = [
                            prev_center[0] + velocity[0],  # expected x
                            prev_center[1] + velocity[1],  # expected y
                        ]
                        logger.debug(f"Frame {i}: velocity=[{velocity[0]:.1f}, {velocity[1]:.1f}], predicted={predicted_center}")
                    
                    # Track: pick detection closest to predicted position
                    best_det = self._pick_tracked_swimmer(
                        all_detections, prev_bbox, prev_center, target_lane,
                        predicted_center=predicted_center,  # –ü–µ—Ä–µ–¥–∞—ë–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ!
                        velocity=velocity
                    )
            
            best_det["frame_index"] = i
            best_det["frame_path"] = frame_path
            best_det["timestamp"] = timestamp  # –†–µ–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –∏–∑ –≤–∏–¥–µ–æ (—Å–µ–∫)
            best_det["video_frame"] = video_frame  # –ù–æ–º–µ—Ä –∫–∞–¥—Ä–∞ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º –≤–∏–¥–µ–æ
            results.append(best_det)
            
            # Update tracking state with velocity
            prev_prev_center = prev_center  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ velocity
            prev_bbox = best_det.get("bbox")
            prev_center = best_det.get("center")
            if best_det.get("lane"):
                target_lane = best_det["lane"]
        
        # –ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∫–∞–¥—Ä–æ–≤ (–≤–∞–∂–Ω–æ –¥–ª—è –ø–æ–¥–≤–æ–¥–Ω—ã—Ö —Å—Ü–µ–Ω)
        results = self._interpolate_missing_detections(results)
        
        # –°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ (—É–±–∏—Ä–∞–µ—Ç "–ø—å—è–Ω—ã–π" —ç—Ñ—Ñ–µ–∫—Ç)
        results = self._smooth_trajectory(results, window_size=5)
        
        return results
    
    def _detect_all_persons(
        self, frame_path: str, confidence_threshold: float
    ) -> List[Dict]:
        """Get all person detections in a frame.
        
        Args:
            frame_path: Path to frame
            confidence_threshold: Minimum confidence
            
        Returns:
            List of all person detections with bbox, confidence, center, lane
        """
        frame = cv2.imread(frame_path)
        if frame is None:
            return []
        
        height, width = frame.shape[:2]
        results = self.model(frame, verbose=False)
        
        detections = []
        for result in results:
            boxes = result.boxes
            for box in boxes:
                cls = int(box.cls[0])
                conf = float(box.conf[0])
                
                if cls == self.person_class_id and conf >= confidence_threshold:
                    x1, y1, x2, y2 = map(int, box.xyxy[0])
                    cx = (x1 + x2) // 2
                    cy = (y1 + y2) // 2
                    lane = min(8, max(1, int((cx / width) * 8) + 1))
                    
                    detections.append({
                        "bbox": [x1, y1, x2, y2],
                        "confidence": conf,
                        "center": [cx, cy],
                        "lane": lane,
                    })
        
        return detections
    
    def _pick_initial_swimmer(self, detections: List[Dict]) -> Dict:
        """Pick the most likely swimmer from first frame.
        
        Strategy: largest bbox (likely the main subject).
        
        Args:
            detections: All person detections
            
        Returns:
            Best detection
        """
        best_det = None
        best_area = 0
        
        for det in detections:
            bbox = det["bbox"]
            area = (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])
            if area > best_area:
                best_area = area
                best_det = det
        
        return best_det or detections[0]
    
    def _pick_tracked_swimmer(
        self,
        detections: List[Dict],
        prev_bbox: List[int],
        prev_center: List[int],
        target_lane: int,
        predicted_center: List[int] = None,
        velocity: List[float] = None,
    ) -> Dict:
        """Pick detection that best matches the previously tracked swimmer.
        
        Uses IoU, center distance, bbox size similarity, and lane consistency.
        –ù–û–í–û–ï: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç velocity prediction –¥–ª—è —Ç—Ä–µ–∫–∏–Ω–≥–∞ –±—ã—Å—Ç—Ä—ã—Ö –ø–ª–æ–≤—Ü–æ–≤.
        
        Args:
            detections: All person detections in current frame
            prev_bbox: Previous frame bbox
            prev_center: Previous frame center
            target_lane: Expected lane number
            predicted_center: Predicted center position (from velocity)
            velocity: Movement velocity [vx, vy] in pixels/frame
            
        Returns:
            Best matching detection
        """
        best_det = None
        best_score = -1
        best_iou = 0
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–º–µ—Ä –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ bbox –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        prev_width = prev_bbox[2] - prev_bbox[0]
        prev_height = prev_bbox[3] - prev_bbox[1]
        prev_area = prev_width * prev_height
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å–∫–æ—Ä–æ—Å—Ç—å –¥–≤–∏–∂–µ–Ω–∏—è –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ search radius
        speed = 0
        if velocity:
            speed = np.sqrt(velocity[0]**2 + velocity[1]**2)  # –ü–∏–∫—Å–µ–ª–µ–π/–∫–∞–¥—Ä
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π max distance –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å–∫–æ—Ä–æ—Å—Ç–∏
        # –ë—ã—Å—Ç—Ä—ã–µ –ø–ª–æ–≤—Ü—ã: –±–æ–ª—å—à–µ search radius
        base_max_dist = 150
        adaptive_max_dist = base_max_dist + speed * 1.5  # –†–∞—Å—à–∏—Ä—è–µ–º –∑–æ–Ω—É –ø–æ–∏—Å–∫–∞
        logger.debug(f"Speed: {speed:.1f}px/frame, search radius: {adaptive_max_dist:.1f}px")
        
        for det in detections:
            score = 0.0
            
            # 1. IoU similarity (0-1) - –°–ê–ú–´–ô –í–ê–ñ–ù–´–ô –¥–ª—è –æ–¥–Ω–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞!
            iou = self._calculate_iou(prev_bbox, det["bbox"])
            score += iou * 10.0  # Weight: 10x (–±—ã–ª–æ 3x) ‚Üê —É–≤–µ–ª–∏—á–µ–Ω–æ!
            
            # 2. Center distance (inverse, normalized)
            # –ù–û–í–û–ï: –∏—Å–ø–æ–ª—å–∑—É–µ–º predicted_center –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω!
            reference_center = predicted_center if predicted_center else prev_center
            
            if reference_center and det["center"]:
                dx = det["center"][0] - reference_center[0]
                dy = det["center"][1] - reference_center[1]
                dist = np.sqrt(dx**2 + dy**2)
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π max distance (—Ä–∞—Å—à–∏—Ä—è–µ—Ç—Å—è –¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö –ø–ª–æ–≤—Ü–æ–≤)
                dist_score = max(0, 1.0 - dist / adaptive_max_dist)
                score += dist_score * 5.0  # Weight: 5x
                
                # –ë–æ–Ω—É—Å –µ—Å–ª–∏ –¥–µ—Ç–µ–∫—Ü–∏—è –±–ª–∏–∑–∫–æ –∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–π –ø–æ–∑–∏—Ü–∏–∏
                if predicted_center and dist < speed * 0.5:  # –í –ø—Ä–µ–¥–µ–ª–∞—Ö –ø–æ–ª–æ–≤–∏–Ω—ã —Å–∫–æ—Ä–æ—Å—Ç–∏
                    score += 2.0  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –±–æ–Ω—É—Å –∑–∞ —Ç–æ—á–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ!
            
            # 3. –†–∞–∑–º–µ—Ä bbox (–ø–æ—Ö–æ–∂–∏–π —Ä–∞–∑–º–µ—Ä = —Ç–æ—Ç –∂–µ –æ–±—ä–µ–∫—Ç)
            det_width = det["bbox"][2] - det["bbox"][0]
            det_height = det["bbox"][3] - det["bbox"][1]
            det_area = det_width * det_height
            
            # –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –ø–ª–æ—â–∞–¥–µ–π (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –±–ª–∏–∑–∫–æ –∫ 1.0)
            area_ratio = min(prev_area, det_area) / max(prev_area, det_area) if max(prev_area, det_area) > 0 else 0
            # –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Å—Ç–æ—Ä–æ–Ω (aspect ratio)
            prev_aspect = prev_width / prev_height if prev_height > 0 else 1
            det_aspect = det_width / det_height if det_height > 0 else 1
            aspect_similarity = min(prev_aspect, det_aspect) / max(prev_aspect, det_aspect) if max(prev_aspect, det_aspect) > 0 else 0
            
            # Bonus –∑–∞ –ø–æ—Ö–æ–∂–∏–π —Ä–∞–∑–º–µ—Ä –∏ —Ñ–æ—Ä–º—É
            size_score = (area_ratio + aspect_similarity) / 2.0
            score += size_score * 3.0  # Weight: 3x
            
            # 4. Lane consistency
            if target_lane and det["lane"] == target_lane:
                score += 2.0  # Weight: 2x (–±—ã–ª–æ 1x) ‚Üê —É–≤–µ–ª–∏—á–µ–Ω–æ!
            
            # 5. Confidence bonus (–º–µ–Ω—å—à–∏–π –≤–µ—Å - –Ω–µ –≥–ª–∞–≤–Ω–æ–µ)
            score += det["confidence"] * 0.3  # Weight: 0.3x (–±—ã–ª–æ 0.5x)
            
            if score > best_score:
                best_score = score
                best_iou = iou
                best_det = det
        
        # –ñ–ï–°–¢–ö–ò–ô –ü–û–†–û–ì: –µ—Å–ª–∏ –ª—É—á—à–∏–π IoU —Å–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∏–π, –≤–æ–∑–º–æ–∂–Ω–æ —ç—Ç–æ –¥—Ä—É–≥–æ–π –æ–±—ä–µ–∫—Ç!
        # –ù–µ –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –¥—Ä—É–≥–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞ –µ—Å–ª–∏ overlap < 20%
        if best_iou < 0.2 and best_det is not None:
            logger.warning(f"Low IoU {best_iou:.3f} - possible object switch! Keeping previous bbox.")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º "—Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫—É—é" –¥–µ—Ç–µ–∫—Ü–∏—é —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º bbox
            # –≠—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç –ø–µ—Ä–µ—Å–∫–∞–∫–∏–≤–∞–Ω–∏–µ
            return {
                "bbox": prev_bbox,
                "center": prev_center,
                "confidence": 0.5,
                "lane": target_lane,
                "tracking_lost": True  # –ü–æ–º–µ—á–∞–µ–º —á—Ç–æ —Ç—Ä–µ–∫–∏–Ω–≥ –Ω–µ–Ω–∞–¥–µ–∂–µ–Ω
            }
        
        # Fallback: return highest confidence if tracking failed
        if best_det is None:
            best_det = max(detections, key=lambda d: d["confidence"])
            logger.warning("Tracking fallback: using highest confidence detection")
        
        logger.debug(f"Tracking score: {best_score:.2f}, IoU: {best_iou:.3f}")
        return best_det
    
    def _calculate_iou(self, bbox1: List[int], bbox2: List[int]) -> float:
        """Calculate Intersection over Union between two bboxes.
        
        Args:
            bbox1: [x1, y1, x2, y2]
            bbox2: [x1, y1, x2, y2]
            
        Returns:
            IoU value (0-1)
        """
        x1 = max(bbox1[0], bbox2[0])
        y1 = max(bbox1[1], bbox2[1])
        x2 = min(bbox1[2], bbox2[2])
        y2 = min(bbox1[3], bbox2[3])
        
        if x2 < x1 or y2 < y1:
            return 0.0
        
        intersection = (x2 - x1) * (y2 - y1)
        area1 = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])
        area2 = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])
        union = area1 + area2 - intersection
        
        return intersection / union if union > 0 else 0.0

    def draw_detections(
        self,
        frame_path: str,
        detection: Dict,
        output_path: str = None,
    ) -> str:
        """Draw bounding box on frame.

        Args:
            frame_path: Input frame path
            detection: Detection result
            output_path: Output path (default: add _detected suffix)

        Returns:
            Path to output image
        """
        frame = cv2.imread(frame_path)

        if detection["bbox"] is not None:
            x1, y1, x2, y2 = detection["bbox"]
            conf = detection["confidence"]
            lane = detection["lane"]

            # Draw bounding box
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)

            # Draw label
            label = f"Swimmer (Lane {lane}): {conf:.2f}"
            (label_w, label_h), _ = cv2.getTextSize(
                label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2
            )
            cv2.rectangle(
                frame,
                (x1, y1 - label_h - 10),
                (x1 + label_w, y1),
                (0, 255, 0),
                -1,
            )
            cv2.putText(
                frame,
                label,
                (x1, y1 - 5),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.6,
                (0, 0, 0),
                2,
            )

            # Draw center point
            cx, cy = detection["center"]
            cv2.circle(frame, (cx, cy), 5, (0, 0, 255), -1)

        # Save output
        if output_path is None:
            path = Path(frame_path)
            output_path = str(path.parent / f"{path.stem}_detected{path.suffix}")

        cv2.imwrite(output_path, frame)
        logger.info(f"Saved detection image: {output_path}")

        return output_path

    def save_detections_json(
        self,
        detections: List[Dict],
        output_path: str = "detections.json",
    ) -> str:
        """Save detections to JSON file.

        Args:
            detections: List of detection results
            output_path: Output JSON path

        Returns:
            Path to saved JSON
        """
        # Remove non-serializable frame objects
        clean_detections = []
        for det in detections:
            clean_det = {
                "frame_index": det["frame_index"],
                "bbox": det["bbox"],
                "confidence": det["confidence"],
                "center": det["center"],
                "lane": det["lane"],
            }
            clean_detections.append(clean_det)

        with open(output_path, "w") as f:
            json.dump(clean_detections, f, indent=2)

        logger.info(f"Saved detections to {output_path}")
        return output_path
    
    def _interpolate_missing_detections(self, detections: List[Dict]) -> List[Dict]:
        """–ò–Ω—Ç–µ—Ä–ø–æ–ª–∏—Ä–æ–≤–∞—Ç—å bbox –¥–ª—è –∫–∞–¥—Ä–æ–≤ –≥–¥–µ –¥–µ—Ç–µ–∫—Ü–∏—è –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∞.
        
        –û—Å–æ–±–µ–Ω–Ω–æ –≤–∞–∂–Ω–æ –¥–ª—è –ø–æ–¥–≤–æ–¥–Ω—ã—Ö —Å—Ü–µ–Ω –≥–¥–µ YOLO –º–æ–∂–µ—Ç –Ω–µ –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–ª–æ–≤—Ü–∞,
        –Ω–æ –º—ã –∑–Ω–∞–µ–º —á—Ç–æ –æ–Ω —Ç–∞–º –µ—Å—Ç—å –ø–æ —Å–æ—Å–µ–¥–Ω–∏–º –∫–∞–¥—Ä–∞–º.
        
        Args:
            detections: –°–ø–∏—Å–æ–∫ –¥–µ—Ç–µ–∫—Ü–∏–π —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏ (bbox=None)
            
        Returns:
            –°–ø–∏—Å–æ–∫ –¥–µ—Ç–µ–∫—Ü–∏–π —Å –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–º–∏ –ø—Ä–æ–ø—É—Å–∫–∞–º–∏
        """
        if not detections:
            return detections
        
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ gaps (–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ None)
        for i in range(len(detections)):
            if detections[i]["bbox"] is not None:
                continue
            
            # –ò—â–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π –≤–∞–ª–∏–¥–Ω—ã–π bbox
            prev_idx = None
            for j in range(i - 1, -1, -1):
                if detections[j]["bbox"] is not None:
                    prev_idx = j
                    break
            
            # –ò—â–µ–º —Å–ª–µ–¥—É—é—â–∏–π –≤–∞–ª–∏–¥–Ω—ã–π bbox
            next_idx = None
            for j in range(i + 1, len(detections)):
                if detections[j]["bbox"] is not None:
                    next_idx = j
                    break
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å –æ–±–∞ - –∏–Ω—Ç–µ—Ä–ø–æ–ª–∏—Ä—É–µ–º
            if prev_idx is not None and next_idx is not None:
                prev_bbox = detections[prev_idx]["bbox"]
                next_bbox = detections[next_idx]["bbox"]
                
                # –õ–∏–Ω–µ–π–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è
                gap_size = next_idx - prev_idx
                position = i - prev_idx
                ratio = position / gap_size
                
                interpolated_bbox = [
                    int(prev_bbox[0] + (next_bbox[0] - prev_bbox[0]) * ratio),
                    int(prev_bbox[1] + (next_bbox[1] - prev_bbox[1]) * ratio),
                    int(prev_bbox[2] + (next_bbox[2] - prev_bbox[2]) * ratio),
                    int(prev_bbox[3] + (next_bbox[3] - prev_bbox[3]) * ratio),
                ]
                
                cx = (interpolated_bbox[0] + interpolated_bbox[2]) // 2
                cy = (interpolated_bbox[1] + interpolated_bbox[3]) // 2
                
                detections[i]["bbox"] = interpolated_bbox
                detections[i]["center"] = [cx, cy]
                detections[i]["confidence"] = 0.5  # –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏
                detections[i]["interpolated"] = True  # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ –∏–Ω—Ç–µ—Ä–ø–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π
                
                logger.debug(f"Interpolated bbox for frame {i} (underwater gap)")
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å —Ç–æ–ª—å–∫–æ prev - —ç–∫—Å—Ç—Ä–∞–ø–æ–ª–∏—Ä—É–µ–º –≤–ø–µ—Ä—ë–¥ (–¥–æ 3 –∫–∞–¥—Ä–æ–≤)
            elif prev_idx is not None and (i - prev_idx) <= 3:
                detections[i]["bbox"] = detections[prev_idx]["bbox"]
                detections[i]["center"] = detections[prev_idx]["center"]
                detections[i]["confidence"] = 0.4
                detections[i]["interpolated"] = True
                logger.debug(f"Extrapolated bbox forward for frame {i}")
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å —Ç–æ–ª—å–∫–æ next - —ç–∫—Å—Ç—Ä–∞–ø–æ–ª–∏—Ä—É–µ–º –Ω–∞–∑–∞–¥ (–¥–æ 3 –∫–∞–¥—Ä–æ–≤)
            elif next_idx is not None and (next_idx - i) <= 3:
                detections[i]["bbox"] = detections[next_idx]["bbox"]
                detections[i]["center"] = detections[next_idx]["center"]
                detections[i]["confidence"] = 0.4
                detections[i]["interpolated"] = True
                logger.debug(f"Extrapolated bbox backward for frame {i}")
        
        return detections
    
    def _smooth_trajectory(
        self, detections: List[Dict], window_size: int = 5
    ) -> List[Dict]:
        """–°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ bbox –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –¥—Ä–æ–∂–∞–Ω–∏—è.
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ –¥–ª—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç bbox,
        —á—Ç–æ —É–±–∏—Ä–∞–µ—Ç "–ø—å—è–Ω—ã–π" —ç—Ñ—Ñ–µ–∫—Ç –∏ –∑–∞–ø–∞–∑–¥—ã–≤–∞–Ω–∏—è.
        
        Args:
            detections: –°–ø–∏—Å–æ–∫ –¥–µ—Ç–µ–∫—Ü–∏–π
            window_size: –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –¥–ª—è —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ —Å—Ä–µ–¥–Ω–µ–≥–æ (–Ω–µ—á—ë—Ç–Ω–æ–µ —á–∏—Å–ª–æ)
            
        Returns:
            –°–ø–∏—Å–æ–∫ –¥–µ—Ç–µ–∫—Ü–∏–π —Å–æ —Å–≥–ª–∞–∂–µ–Ω–Ω—ã–º–∏ bbox
        """
        if not detections or window_size < 3:
            return detections
        
        # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ window_size –Ω–µ—á—ë—Ç–Ω—ã–π
        if window_size % 2 == 0:
            window_size += 1
        
        half_window = window_size // 2
        smoothed_detections = []
        
        for i, detection in enumerate(detections):
            if detection["bbox"] is None:
                smoothed_detections.append(detection)
                continue
            
            # –°–æ–±–∏—Ä–∞–µ–º bbox –∏–∑ –æ–∫–Ω–∞ –≤–æ–∫—Ä—É–≥ —Ç–µ–∫—É—â–µ–≥–æ –∫–∞–¥—Ä–∞
            window_bboxes = []
            for j in range(max(0, i - half_window), min(len(detections), i + half_window + 1)):
                if detections[j]["bbox"] is not None:
                    window_bboxes.append(detections[j]["bbox"])
            
            if not window_bboxes:
                smoothed_detections.append(detection)
                continue
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–π –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
            avg_bbox = [
                int(sum(bbox[0] for bbox in window_bboxes) / len(window_bboxes)),  # x1
                int(sum(bbox[1] for bbox in window_bboxes) / len(window_bboxes)),  # y1
                int(sum(bbox[2] for bbox in window_bboxes) / len(window_bboxes)),  # x2
                int(sum(bbox[3] for bbox in window_bboxes) / len(window_bboxes)),  # y2
            ]
            
            # –û–±–Ω–æ–≤–ª—è–µ–º center
            cx = (avg_bbox[0] + avg_bbox[2]) // 2
            cy = (avg_bbox[1] + avg_bbox[3]) // 2
            
            # –°–æ–∑–¥–∞—ë–º —Å–≥–ª–∞–∂–µ–Ω–Ω—É—é –¥–µ—Ç–µ–∫—Ü–∏—é
            smoothed_det = detection.copy()
            smoothed_det["bbox"] = avg_bbox
            smoothed_det["center"] = [cx, cy]
            smoothed_det["smoothed"] = True
            
            smoothed_detections.append(smoothed_det)
            
            if i % 50 == 0:
                logger.debug(f"Smoothed bbox for frame {i}: window={len(window_bboxes)} bboxes")
        
        logger.info(f"Trajectory smoothing applied (window_size={window_size})")
        return smoothed_detections
    
    def _estimate_lane(self, center_x: int, frame_width: int) -> int:
        """–û—Ü–µ–Ω–∏—Ç—å –Ω–æ–º–µ—Ä –¥–æ—Ä–æ–∂–∫–∏ –ø–æ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–æ–π –ø–æ–∑–∏—Ü–∏–∏.
        
        Args:
            center_x: X –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞ —Ü–µ–Ω—Ç—Ä–∞ –ø–ª–æ–≤—Ü–∞
            frame_width: –®–∏—Ä–∏–Ω–∞ –∫–∞–¥—Ä–∞
            
        Returns:
            Lane number (1-8)
        """
        # –î–µ–ª–∏–º —à–∏—Ä–∏–Ω—É –∫–∞–¥—Ä–∞ –Ω–∞ 8 –¥–æ—Ä–æ–∂–µ–∫
        lane = min(8, max(1, int((center_x / frame_width) * 8) + 1))
        return lane
    
    def _detect_underwater_with_motion(
        self,
        frame_path: str,
        prev_frame_path: str = None,
        search_bbox: List[int] = None,
    ) -> Dict:
        """–î–µ—Ç–µ–∫—Ü–∏—è –ø–æ–¥ –≤–æ–¥–æ–π –∏—Å–ø–æ–ª—å–∑—É—è motion detection.
        
        –ö–æ–≥–¥–∞ YOLO –Ω–µ –≤–∏–¥–∏—Ç –ø–ª–æ–≤—Ü–∞ –ø–æ–¥ –≤–æ–¥–æ–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑–Ω–∏—Ü—É –∫–∞–¥—Ä–æ–≤
        –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏—è –≤ –æ–∂–∏–¥–∞–µ–º–æ–π –æ–±–ª–∞—Å—Ç–∏.
        
        Args:
            frame_path: –¢–µ–∫—É—â–∏–π –∫–∞–¥—Ä
            prev_frame_path: –ü—Ä–µ–¥—ã–¥—É—â–∏–π –∫–∞–¥—Ä –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            search_bbox: –û–±–ª–∞—Å—Ç—å –ø–æ–∏—Å–∫–∞ [x1, y1, x2, y2]
            
        Returns:
            Detection dict –∏–ª–∏ None
        """
        if not prev_frame_path:
            return None
        
        frame = cv2.imread(frame_path)
        prev_frame = cv2.imread(prev_frame_path)
        
        if frame is None or prev_frame is None:
            return None
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ grayscale
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å search_bbox, –∏—â–µ–º —Ç–æ–ª—å–∫–æ —Ç–∞–º
        if search_bbox:
            x1, y1, x2, y2 = search_bbox
            x1, y1 = max(0, x1), max(0, y1)
            x2, y2 = min(frame.shape[1], x2), min(frame.shape[0], y2)
            
            gray_roi = gray[y1:y2, x1:x2]
            prev_gray_roi = prev_gray[y1:y2, x1:x2]
        else:
            gray_roi = gray
            prev_gray_roi = prev_gray
            x1, y1 = 0, 0
        
        # Frame difference –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏—è
        diff = cv2.absdiff(gray_roi, prev_gray_roi)
        
        # Threshold –¥–ª—è –±–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏–∏
        _, thresh = cv2.threshold(diff, 25, 255, cv2.THRESH_BINARY)
        
        # –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è —à—É–º–∞
        kernel = np.ones((5, 5), np.uint8)
        thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)
        thresh = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)
        
        # –ù–∞—Ö–æ–¥–∏–º –∫–æ–Ω—Ç—É—Ä—ã –¥–≤–∏–∂—É—â–∏—Ö—Å—è –æ–±—ä–µ–∫—Ç–æ–≤
        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        if not contours:
            return None
        
        # –ë–µ—Ä—ë–º —Å–∞–º—ã–π –±–æ–ª—å—à–æ–π –∫–æ–Ω—Ç—É—Ä (–≤–µ—Ä–æ—è—Ç–Ω–æ –ø–ª–æ–≤–µ—Ü)
        largest_contour = max(contours, key=cv2.contourArea)
        area = cv2.contourArea(largest_contour)
        
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–ª–æ—â–∞–¥—å –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ (—Ñ–∏–ª—å—Ç—Ä—É–µ–º —à—É–º)
        min_area = 5000  # –ø–∏–∫—Å–µ–ª–µ–π
        if area < min_area:
            logger.debug(f"Motion detected but area too small: {area}px < {min_area}px")
            return None
        
        # –ü–æ–ª—É—á–∞–µ–º bounding box –∫–æ–Ω—Ç—É—Ä–∞
        x, y, w, h = cv2.boundingRect(largest_contour)
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –ø–æ–ª–Ω–æ–≥–æ –∫–∞–¥—Ä–∞
        bbox = [x1 + x, y1 + y, x1 + x + w, y1 + y + h]
        center = [bbox[0] + (bbox[2] - bbox[0]) // 2, bbox[1] + (bbox[3] - bbox[1]) // 2]
        
        logger.info(f"üåä Underwater motion detected: area={area}px, bbox={bbox}")
        
        return {
            "bbox": bbox,
            "center": center,
            "confidence": 0.5,  # Medium confidence –¥–ª—è motion-based
            "lane": self._estimate_lane(center[0], frame.shape[1]),
            "underwater_detection": True,  # –§–ª–∞–≥ —á—Ç–æ —ç—Ç–æ underwater detection
        }
    
    def _aggressive_extrapolate(
        self,
        prev_bbox: List[int],
        prev_center: List[int],
        velocity: List[float],
        frames_missing: int = 1,
        target_lane: int = None,
        frame_width: int = 1920,
    ) -> Dict:
        """–ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è bbox –ø–æ velocity –¥–ª—è –ø–æ–¥–≤–æ–¥–Ω—ã—Ö —É—á–∞—Å—Ç–∫–æ–≤.
        
        Args:
            prev_bbox: –ü–æ—Å–ª–µ–¥–Ω–∏–π –∏–∑–≤–µ—Å—Ç–Ω—ã–π bbox
            prev_center: –ü–æ—Å–ª–µ–¥–Ω–∏–π –∏–∑–≤–µ—Å—Ç–Ω—ã–π center
            velocity: –°–∫–æ—Ä–æ—Å—Ç—å –¥–≤–∏–∂–µ–Ω–∏—è [vx, vy]
            frames_missing: –°–∫–æ–ª—å–∫–æ –∫–∞–¥—Ä–æ–≤ –ø—Ä–æ–ø—É—â–µ–Ω–æ
            target_lane: –ò–∑–≤–µ—Å—Ç–Ω–∞—è –¥–æ—Ä–æ–∂–∫–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
            frame_width: –®–∏—Ä–∏–Ω–∞ –∫–∞–¥—Ä–∞ (–¥–ª—è –æ—Ü–µ–Ω–∫–∏ lane –µ—Å–ª–∏ target_lane=None)
            
        Returns:
            Extrapolated detection dict
        """
        # –≠–∫—Å—Ç—Ä–∞–ø–æ–ª–∏—Ä—É–µ–º center
        new_center = [
            int(prev_center[0] + velocity[0] * frames_missing),
            int(prev_center[1] + velocity[1] * frames_missing),
        ]
        
        # –≠–∫—Å—Ç—Ä–∞–ø–æ–ª–∏—Ä—É–µ–º bbox (—Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä)
        bbox_width = prev_bbox[2] - prev_bbox[0]
        bbox_height = prev_bbox[3] - prev_bbox[1]
        
        new_bbox = [
            new_center[0] - bbox_width // 2,
            new_center[1] - bbox_height // 2,
            new_center[0] + bbox_width // 2,
            new_center[1] + bbox_height // 2,
        ]
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–∑–≤–µ—Å—Ç–Ω—É—é lane –µ—Å–ª–∏ –µ—Å—Ç—å, –∏–Ω–∞—á–µ –æ—Ü–µ–Ω–∏–≤–∞–µ–º
        lane = target_lane if target_lane else self._estimate_lane(new_center[0], frame_width)
        
        logger.debug(f"‚ö° Aggressive extrapolation: center={new_center}, velocity={velocity}")
        
        return {
            "bbox": new_bbox,
            "center": new_center,
            "confidence": 0.3,  # Low confidence –¥–ª—è extrapolated
            "lane": lane,
            "extrapolated": True,  # –§–ª–∞–≥ —á—Ç–æ —ç—Ç–æ —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è
        }


def detect_swimmer_in_frames(
    frame_paths: List[str],
    output_dir: str = "./detections",
    draw_boxes: bool = True,
    enable_tracking: bool = True,
) -> Dict:
    """Detect swimmer in frames (convenience function).

    Args:
        frame_paths: List of frame paths
        output_dir: Output directory for results
        draw_boxes: Whether to draw bounding boxes
        enable_tracking: If True, track same swimmer across frames (recommended for multi-person scenes)

    Returns:
        Dict with detection results and paths
    """
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    detector = SwimmerDetector()
    detections = detector.detect_batch(frame_paths, enable_tracking=enable_tracking)

    # Save JSON
    json_path = detector.save_detections_json(
        detections,
        str(output_path / "detections.json"),
    )

    # Draw boxes
    detected_images = []
    if draw_boxes:
        for detection in detections:
            if detection["bbox"] is not None:
                img_path = detector.draw_detections(
                    detection["frame_path"],
                    detection,
                    str(output_path / f"detected_{detection['frame_index']:04d}.jpg"),
                )
                detected_images.append(img_path)

    return {
        "detections": detections,
        "json_path": json_path,
        "detected_images": detected_images,
        "output_dir": str(output_path),
    }


if __name__ == "__main__":
    # Example usage
    logging.basicConfig(level=logging.INFO)

    # Test on sample frames
    frame_paths = [f"./frames/frame_{i:04d}.jpg" for i in range(10)]

    result = detect_swimmer_in_frames(
        frame_paths,
        output_dir="./detection_results",
    )

    print(f"Detected in {len(result['detections'])} frames")
    print(f"Results saved to {result['output_dir']}")
